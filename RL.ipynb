{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for imbalanced 10-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import gym\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_training-set.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "df = pd.concat([train,test]).drop(['id', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOMINAL_COLS = ['proto', 'service', 'state']\n",
    "BINARY_COLS = ['is_sm_ips_ports', 'is_ftp_login']\n",
    "\n",
    "for name in df.columns:\n",
    "    if name == \"attack_cat\" or name in NOMINAL_COLS or name in BINARY_COLS:\n",
    "        pass\n",
    "    else:\n",
    "        df[name] = zscore(df[name])\n",
    "\n",
    "for name in NOMINAL_COLS:\n",
    "    df = pd.concat([df, pd.get_dummies(df[name], prefix=name)],axis=1)\n",
    "    df.drop(name,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reward bias calculation for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_pc = df['attack_cat'].value_counts(normalize=True).to_frame()\n",
    "props_pc.sort_index(inplace=True)\n",
    "display(props_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_pc['attack_cat'].mean()\n",
    "MINORITY_CLASSES = [0, 1, 2, 4, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_nums = df.groupby('attack_cat')['attack_cat'].count()\n",
    "proportions = att_nums.values\n",
    "display(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_factor = ((max(proportions)/proportions)).astype(int)\n",
    "display(rw_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data normalization, split, optional smote, shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['attack_cat'], axis=1).values\n",
    "\n",
    "# Make the values [0, 1] for the RL environment\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = min_max_scaler.fit_transform(x)\n",
    "\n",
    "dummies = pd.get_dummies(df['attack_cat'])\n",
    "attack_cat_list = list(dummies.columns)\n",
    "y = dummies.values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=42)\n",
    "# x_train, y_train = sm.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(x_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_benchmark = pd.DataFrame(columns=['Accuracy', 'Recall', 'Precision', 'F1-score'])\n",
    "\n",
    "def getMetrics(model_name, y_truth, y_pred, average):\n",
    "    print(f'Metrics {average}-averaged')\n",
    "    acc = metrics.accuracy_score(y_truth, y_pred)\n",
    "    rec = metrics.recall_score(y_truth, y_pred, average=average, zero_division=0)\n",
    "    pre = metrics.precision_score(y_truth, y_pred, average=average, zero_division=0)\n",
    "    f1s = metrics.f1_score(y_truth, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    models_benchmark.loc[model_name] = [acc, rec, pre, f1s]\n",
    "    display(models_benchmark.loc[model_name])\n",
    "    return\n",
    "\n",
    "def getCM(model_name, y_test, y_pred):\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f'{model_name}', fontsize=20)\n",
    "\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', xticklabels=attack_cat_list, yticklabels=attack_cat_list)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Real outcome', fontsize=16)\n",
    "    plt.xlabel('Predicted outcome', fontsize=16)\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, dataset=(x_train, y_train), rw_factor=rw_factor, minority_classes=MINORITY_CLASSES):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.rw_factor = rw_factor\n",
    "        self.idx = 0\n",
    "        self.action_space = gym.spaces.Discrete(self.y.shape[1])\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1,\n",
    "                                                shape=(self.x.shape[1], ),\n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "        self.minority_classes = minority_classes\n",
    "        self.step_counter = 0\n",
    "        self.max_steps = 20e3\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        \n",
    "        # Reward management\n",
    "        if int(action == self.expected_action):\n",
    "            reward = self.rw_factor[self.expected_action]\n",
    "        else:\n",
    "            reward = -self.rw_factor[self.expected_action]\n",
    "                \n",
    "        # Observation management\n",
    "        self.idx += 1\n",
    "        \n",
    "        if self.idx >= self.x.shape[0]:\n",
    "            self.idx = 0\n",
    "        \n",
    "        obs = self.seq_observation()\n",
    "\n",
    "        # Done management\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        if self.step_counter >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_counter = 0\n",
    "        obs = self.seq_observation()\n",
    "        return obs\n",
    "    \n",
    "    def seq_observation(self):        \n",
    "        obs = self.x[self.idx]\n",
    "        self.expected_action = int(np.argmax(self.y[self.idx]))\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback for best model auto-save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBest(BaseCallback):\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 0):\n",
    "        super(SaveBest, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "    \n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    \n",
    "                    if self.verbose >= 0:\n",
    "                        print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                        print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "                        print(f\"Saving new best model to {self.save_path}\")\n",
    "                        \n",
    "                    self.model.save(self.save_path)   \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './tmp_log/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "train_env = CustomEnv()\n",
    "train_env = Monitor(train_env, log_dir)\n",
    "\n",
    "train_env.reset()\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model creation / load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between PPO or A2C, since SB3 only provides vanilla DQN\n",
    "\n",
    "ALGO = 'PPO'\n",
    "callback = SaveBest(check_freq=10e3, log_dir=log_dir)\n",
    "model = PPO(policy='MlpPolicy', env=train_env, tensorboard_log=f'./{ALGO}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO.load(path=f'./models/{ALGO}.zip', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f'./models/{ALGO}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model training / save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min 5M timesteps for relevant results\n",
    "\n",
    "EPOCHS = 20\n",
    "TIMESTEPS = x_train.shape[0]\n",
    "\n",
    "for i in range(1, EPOCHS+1):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f'{ALGO}_TB_LOG', callback=callback)\n",
    "    y_pred = model.predict(x_test)[0]\n",
    "    y_train_pred = model.predict(x_train)[0]\n",
    "    getMetrics(f'RUN{i} - Test', np.argmax(y_test, axis=1), y_pred, 'macro')\n",
    "    getMetrics(f'RUN{i} - Train', np.argmax(y_train, axis=1), y_train_pred, 'macro')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics and CM for the current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMetrics(f'RUN{i} - Train', np.argmax(y_test, axis=1), y_pred, 'macro')\n",
    "print('----- ----- -----')\n",
    "getMetrics(f'RUN{i} - Train', np.argmax(y_test, axis=1), y_pred, 'weighted')\n",
    "getCM('PPO_T', np.argmax(y_test, axis=1), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ker",
   "language": "python",
   "name": "ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1a38fede043c8551f3f80c29646eaec9abea18131b5b9ee9dad7c077f048c74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
