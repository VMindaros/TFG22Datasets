{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for imbalanced 10-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import gym\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_training-set.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "df = pd.concat([train,test]).drop(['id', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOMINAL_COLS = ['proto', 'service', 'state']\n",
    "BINARY_COLS = ['is_sm_ips_ports', 'is_ftp_login']\n",
    "\n",
    "for name in df.columns:\n",
    "    if name == \"attack_cat\" or name in NOMINAL_COLS or name in BINARY_COLS:\n",
    "        pass\n",
    "    else:\n",
    "        df[name] = zscore(df[name])\n",
    "\n",
    "for name in NOMINAL_COLS:\n",
    "    df = pd.concat([df, pd.get_dummies(df[name], prefix=name)],axis=1)\n",
    "    df.drop(name,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reward bias calculation for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of the different att_cat\n",
    "props_pc = df['attack_cat'].value_counts(normalize=True).to_frame()\n",
    "props_pc.sort_index(inplace=True)\n",
    "display(props_pc.style.format({'attack_cat': '{:.2%}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the threshold for one category to be considered minority\n",
    "'{:.2%}'.format(props_pc['attack_cat'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually choose the classes based on index\n",
    "MINORITY_CLASSES = [0, 1, 2, 4, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use each class count to calculate a reward factor inverse to the proportion\n",
    "att_nums = df.groupby('attack_cat')['attack_cat'].count()\n",
    "proportions = att_nums.values\n",
    "display(att_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_factor = ((max(proportions)/proportions)).astype(int)\n",
    "display(rw_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data normalization, split, optional smote, shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['attack_cat'], axis=1).values\n",
    "\n",
    "# Make the values [0, 1] for the RL environment\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = min_max_scaler.fit_transform(x)\n",
    "\n",
    "dummies = pd.get_dummies(df['attack_cat'])\n",
    "attack_cat_list = list(dummies.columns)\n",
    "y = dummies.values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better not use\n",
    "# sm = SMOTE(random_state=42)\n",
    "# x_train, y_train = sm.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(x_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_benchmark = pd.DataFrame(columns=['Accuracy', 'Recall', 'Precision', 'F1-score'])\n",
    "\n",
    "def getMetrics(model_name, y_truth, y_pred, average):\n",
    "    # Macro-avg is recommended to keep minority classes relevant\n",
    "    # Weighted-avg is recommended to keep majority classes relevant\n",
    "    print(f'Metrics {average}-averaged')\n",
    "    acc = metrics.accuracy_score(y_truth, y_pred)\n",
    "    rec = metrics.recall_score(y_truth, y_pred, average=average, zero_division=0)\n",
    "    pre = metrics.precision_score(y_truth, y_pred, average=average, zero_division=0)\n",
    "    f1s = metrics.f1_score(y_truth, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    models_benchmark.loc[model_name] = [acc, rec, pre, f1s]\n",
    "    display(models_benchmark.loc[model_name])\n",
    "    return\n",
    "\n",
    "def getCM(model_name, y_test, y_pred):\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f'{model_name}', fontsize=20)\n",
    "\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', xticklabels=attack_cat_list, yticklabels=attack_cat_list)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Real outcome', fontsize=16)\n",
    "    plt.xlabel('Predicted outcome', fontsize=16)\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, dataset=(x_train, y_train), rw_factor=rw_factor, minority_classes=MINORITY_CLASSES):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.rw_factor = rw_factor\n",
    "        self.idx = 0\n",
    "        self.action_space = gym.spaces.Discrete(self.y.shape[1])\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1,\n",
    "                                                shape=(self.x.shape[1], ),\n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "        self.minority_classes = minority_classes\n",
    "        self.step_counter = 0\n",
    "        self.max_steps = len(self.x)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        \n",
    "        # Reward management\n",
    "        if int(action == self.expected_action):\n",
    "            reward = self.rw_factor[self.expected_action]\n",
    "        else:\n",
    "            reward = -self.rw_factor[self.expected_action]\n",
    "            # Done management 1 -> Stop progressing a bad guess is taken\n",
    "            # on data from a minority class\n",
    "            if self.expected_action in self.minority_classes:\n",
    "                done = True\n",
    "                \n",
    "        # Observation management\n",
    "        self.idx += 1\n",
    "        \n",
    "        if self.idx >= len(self.x):\n",
    "            self.idx = 0\n",
    "        \n",
    "        obs = self.seq_observation()\n",
    "\n",
    "        # Done management 2 -> If a whole sweep of X if achieved\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        if self.step_counter >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Start from a random position in X\n",
    "        self.step_counter = 0\n",
    "        self.idx = random.randint(0, len(self.x) - 1)\n",
    "        obs = self.seq_observation()\n",
    "        return obs\n",
    "    \n",
    "    def seq_observation(self):        \n",
    "        obs = self.x[self.idx]\n",
    "        self.expected_action = int(np.argmax(self.y[self.idx]))\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback for best model auto-save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBest(BaseCallback):\n",
    "    def __init__(self, check_freq: int, max_failure_threshold: int, log_dir: str, verbose: int = 0):\n",
    "        super(SaveBest, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.no_best_count = 0\n",
    "        self.max_failure_threshold = max_failure_threshold\n",
    "    \n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                \n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                \n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    y_train_pred = self.model.predict(x_train)[0]\n",
    "                    getMetrics(f'Training metrics', np.argmax(y_train, axis=1), y_train_pred, 'macro')\n",
    "                    print('\\n')\n",
    "\n",
    "                    if self.verbose >= 0:\n",
    "                        print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                        print(f\"Mean reward per episode: {mean_reward:.2f}\")\n",
    "                        print(f\"Saving new best model to {self.save_path}\")\n",
    "                        print('----- ----- -----')\n",
    "                        \n",
    "                    self.model.save(self.save_path)   \n",
    "                    \n",
    "                else:\n",
    "                    self.no_best_count += 1\n",
    "    \n",
    "                    if self.no_best_count >= self.max_failure_threshold:\n",
    "                        print(f'Restarting best reward: {self.max_failure_threshold} evaluations without improvement')\n",
    "                        self.best_mean_reward = -np.inf\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './tmp_log/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "train_env = CustomEnv()\n",
    "train_env = Monitor(train_env, log_dir)\n",
    "\n",
    "train_env.reset()\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model creation / load / manual save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between PPO or A2C, since SB3 only provides vanilla DQN\n",
    "\n",
    "ALGO = 'PPO'\n",
    "callback = SaveBest(check_freq=1000, max_failure_threshold=2500, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO(policy='MlpPolicy', env=train_env, tensorboard_log=f'./{ALGO}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name to load proper model\n",
    "model = PPO.load(path=f'./tmp_log/PPO_AUTO_BEST.zip', env=train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For manual saving after training\n",
    "\n",
    "# tail = datetime.today().strftime('%d-%m_%H-%M')\n",
    "\n",
    "# model.save(f'./tmp_log/{ALGO}_MANUAL_{tail}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min 5M timesteps for relevant results if training a new model\n",
    "\n",
    "EPOCHS = 1\n",
    "TIMESTEPS = 1e6\n",
    "\n",
    "for i in range(1, EPOCHS+1):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f'{ALGO}_TB_LOG', callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics and CM for the current model on test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)[0]\n",
    "\n",
    "getMetrics('RUN', np.argmax(y_test, axis=1), y_pred, 'macro')\n",
    "print('----- ----- -----')\n",
    "getMetrics('RUN', np.argmax(y_test, axis=1), y_pred, 'weighted')\n",
    "getCM('PPO', np.argmax(y_test, axis=1), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ker",
   "language": "python",
   "name": "ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1a38fede043c8551f3f80c29646eaec9abea18131b5b9ee9dad7c077f048c74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
